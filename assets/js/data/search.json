[ { "title": "Alkemy challenge", "url": "/posts/alkemy/", "categories": "Projects, Pandas", "tags": "transform, data, python, pandas", "date": "2022-03-13 18:32:00 -0300", "snippet": "To be started… Understand data Prepare data Clean dataAboutTopicFeaturesExploring and filtering dataFinal result" }, { "title": "Pandas + Power BI - Properati", "url": "/posts/pbi-properati/", "categories": "Projects, Pandas+PowerBI", "tags": "transform, data, python, powerbi", "date": "2022-02-18 18:32:00 -0300", "snippet": " Understand data Prepare data Clean data Remove unnecessary data Fix time series related data Replace NaNs by real and useful data Build PBI dashboardAboutThis dataset comes from Properati Data (https://www.properati.com.ar/data/) is the data division of Properati, a real estate search site in Latin America.The idea was to work 100% from PowerBI but I ran into difficulties (power bi is very slow for some tasks) so I decided to first clean the data with Pandas and then use PowerBI for graphics I will be updating this page at least once a weekTopicIt contains information on online property rental or sale ads from around 2020 to 2021, it has various details such as square meters, price, region, etc.Features Col Description type Notice type (Property, Development/Project) country Country in which the notice is published id Notice identifier. It is not unique: if the notice is updated by the real estate agency (new version of the notice), a new record is created with the same id but different dates start_date Notice registration date end_date Notice cancellation date place Fields referring to the location of the property or development lat Latitude lon Length l1,l2,l3,l4 Country, province, city, neighborhood operation Type of operation (Sale, Rent) type Type of property (House, Department, PH) rooms Number of rooms (useful in Argentina) bedrooms Number of bedrooms (useful in the rest of the countries) bathrooms Number of bathrooms surface_total Total surface in m² surface_covered Surface covered in m² price Price currency Currency of the published price price_period Price Period (Daily, Weekly, Monthly) title Title of the ad description Description of the ad development Fields related to real estate development (empty if the ad is for a property) status Development status (Completed, Under construction, …) name Development name short_description Short description of the ad description Description of the ad Exploring and filtering dataimport pandas as pddf = pd.read_csv(&#39;ar_properties.csv&#39;)df.head()df.info() RangeIndex: 1000000 entries, 0 to 999999 Data columns (total 25 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 id 1000000 non-null object 1 ad_type 1000000 non-null object 2 start_date 1000000 non-null object 3 end_date 1000000 non-null object 4 created_on 1000000 non-null object 5 lat 894233 non-null float64 6 lon 894191 non-null float64 7 l1 1000000 non-null object 8 l2 1000000 non-null object 9 l3 965273 non-null object 10 l4 306162 non-null object 11 l5 5530 non-null object 12 l6 0 non-null float64 13 rooms 714179 non-null float64 14 bedrooms 649933 non-null float64 15 bathrooms 765122 non-null float64 16 surface_total 477831 non-null float64 17 surface_covered 487756 non-null float64 18 price 958243 non-null float64 19 currency 955491 non-null object 20 price_period 429870 non-null object 21 title 999999 non-null object 22 description 999958 non-null object 23 property_type 1000000 non-null object 24 operation_type 1000000 non-null object dtypes: float64(9), object(16) memory usage: 190.7+ MBWe have a 190MB memory usage dataframe, we will be dropping the following columns: ID: is not needed because it can be replaced by the dataframe index created_on: is the same as start_date L5, L6: have a lot of nulls title, description: has text which i will not use for this notebook This improves memory usage about 20%df = df.drop([&#39;title&#39;, &#39;description&#39;,&#39;id&#39;,&#39;created_on&#39;,&#39;l5&#39;,&#39;l6&#39;], axis = 1)print(&#39;memory usage: &#39;+str(df.memory_usage().sum()//1024000)+&#39; MB&#39;)-&amp;gt;&amp;gt;&amp;gt; memory usage: 148 MB # aprox. -20% memory usageWhat is inside each column? More or less 50% float 50% text…df.dtypes ad_type object start_date object end_date object lat float64 lon float64 l1 object l2 object l3 object l4 object rooms float64 bedrooms float64 bathrooms float64 surface_total float64 surface_covered float64 price float64 currency object price_period object property_type object operation_type objectWe can later improve the dataframe memory usage transforming some columns to Categorical data typedf.nunique() ad_type 1 start_date 362 end_date 450 lat 389394 lon 392553 l1 4 l2 43 l3 1367 l4 1070 rooms 38 bedrooms 85 bathrooms 20 surface_total 5191 surface_covered 3370 price 20807 currency 5 price_period 3 property_type 10 operation_type 3 dtype: int64Nulls for each column (%)df.isna().mean()*100).round(1) ad_type 0.0 start_date 0.0 end_date 0.0 lat 10.6 lon 10.6 l1 0.0 l2 0.0 l3 3.5 l4 69.4 rooms 28.6 bedrooms 35.0 bathrooms 23.5 surface_total 52.2 surface_covered 51.2 price 4.2 currency 4.5 price_period 57.0 property_type 0.0 operation_type 0.0Drop L4 because we can locate using L1, L2, L3, lat and londf = df.drop([&#39;l4&#39;],axis=1)df.head(3)Transforming start_date and end_date to date format but… we ran into an error! some dates are formated as 9999-12-31 00:00:00df[&#39;start_date&#39;] = pd.to_datetime(df[&#39;start_date&#39;])df[&#39;end_date&#39;] = pd.to_datetime(df[&#39;end_date&#39;])df.head(3)ERROR: OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 9999-12-31 00:00:00Let’s fix it replacing 9999 by start_date YYYY :-)import numpy as npdf[&#39;end_date&#39;] = np.where(df.end_date.str[:4] == &#39;9999&#39;, df.start_date.str[:4] + df.end_date.str[4:], df.end_date)Check how many unique values are presentcat_cols = [&#39;l1&#39;,&#39;currency&#39;,&#39;price_period&#39;, &#39;property_type&#39;, &#39;operation_type&#39;]df.select_dtypes(include=object).columns.tolist()(pd.DataFrame(df[cat_cols].melt(var_name=&#39;column&#39;, value_name=&#39;value&#39;).value_counts()).rename(columns={0:&#39;qty&#39;}).sort_values(by=[&#39;column&#39;, &#39;qty&#39;])) qty column value currency COP 3 PEN 292 UYU 2785 ARS 214254 USD 738157 l1 Brasil 244 Estados Unidos 892 Uruguay 15749 Argentina 983115 operation_type Alquiler temporal 41679 Alquiler 207490 Venta 750831 price_period Diario 11 Semanal 17 Mensual 429842 property_type Casa de campo 2251 Depósito 7872 Cochera 13276 Otro 27837 Oficina 32390 Local comercial 45593 PH 45837 Lote 121311 Casa 245726 Departamento 457907 We will only focus on Argentinan market, also drop rows with low amount of valuesCurrencyars_usd_currency = df[ (df[&#39;currency&#39;] != &#39;ARS&#39;) &amp;amp; (df[&#39;currency&#39;] != &#39;USD&#39;) ].indexdf.drop(ars_usd_currency,inplace=True)L1L1_only_argentina = df[ (df[&#39;l1&#39;] != &#39;Argentina&#39;)].indexdf.drop(L1_only_argentina,inplace=True)Some rows in LAT and LON columns have a lot of NaN values, we will be replacing NaNs using the location in L3 L4 columns, steps followed: Create the a new df by filtering only null values in LAT column df_NaN_lat_lon = df.loc[(df[&#39;lat&#39;].isnull())] Download the new df as a new .csv df_NaN_lat_lon.to_csv(&#39;df_nans.csv&#39;) Pass it to .xls so i can interact much better with it (https://convertio.co/es/csv-xlsx/) Filter that .xls for unique values Use the google datasheet extension ‘ezGeocode’. You pass it the location and returns lat &amp;amp; lon Now it’s time to merge both files and replace nulls for real LAT &amp;amp; LONDashboard!After a few days of work, here is the final dashboard! I tried to make it as simple as possible and self explanatoryThe main idea was to practice the use of PowerBI after cleaning data with Pandas, i am very happy with the final result :-)" }, { "title": "Power BI dashboard - bank marketing campaign", "url": "/posts/pbi-coderhouse/", "categories": "Projects, PowerBI", "tags": "transform, data", "date": "2022-02-02 18:32:00 -0300", "snippet": " Documentation (spanish)AboutThis dashboard was the final project for the data analytics course of the Coderhouse platform. The idea was to take a dataset of your choice and clean it to then organize it into different 5 tables that had to be related to each other (star scheme) The choice was not easy at all since I did not want to fall into a cliche (covid, gdp, environment, etc.)After digging around kaggle for a few days I came across the following dataset which had true data, a good number of rows and many variables to analyze -&amp;gt; Bank mkt campaign datasetTopicThe chosen dataset is related to direct marketing campaigns (phone calls) of a Portuguese banking institutionThe marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be (‘yes’) or not (‘no’) subscribedFeaturesThe dataset contains more than 20 attributes, more than 40K rows and the information is real. All the inputs are from May 2008 to November 2010We can divide all attributes in 4 groups: bank client data (age, job, loans, etc) related to last campaign (type of last contact, month of last contact, etc) social and economic (inflation, bank loan interest, employee rate, etc) other (contacts performed, days from last contact, etc)flowchart TB; subgraph Transforming data into insights; Ask--&amp;gt;Prepare--&amp;gt;Process--&amp;gt;Analyze--&amp;gt;Share--&amp;gt;Act!; end; AskI was interested in knowing how effective the campaign was, that is, if the people it was addressed to were the right ones or if the bank had to adjust its parameters for future campaigns. From this arose the following and many other questions: What age range performed best? Does the income of money affect the result? Does the duration of the calls affect the result? Did the economic crisis of 2008 affect the performance if we compare it with other years? PrepareThe dataset was analyzed in search of information that allows us to answer our questions, once identified, I segmented it and grouped it into several tables. Client data Contact client data Previous and current campaign Socioeconomic variables Calendar tableExamples:The “Client data” table had information exclusive to the client, such as age, type of job, marital status, education, balance and status of default with the bank.The “Socioeconomic variables” table had information on inflation, bank interest, unemployment and consumer confidence ProcessTo clean the data I mostly used PowerBI tools, Power Query Editor, table tools, columns and calculated measuresNull or empty values were removed, the date columns were adapted to the date table, the socioeconomic variables were normalized to refer exclusively to the period of the marketing campaign (thanks OECD)I used calculated columns exclusively to group ages and duration of calls in 3 groups AnalyzeI consider this part the most important since it allows us to show, through graphics and different tools, the story we want to tell.These graphs allowed us to understand much better what relationship exists between the data and to evaluate very quickly what affects the final result, such as the incidence of age or how the decrease or increase in bank interest affects the acquisition of a banking product.For example with donut graphs I was able to quickly compare 8 variables in two different segmentations (current campaign and previous campaign) Final results The duration of the call should exceed 5 minutes The best results are in those classified as single and students (best performance) Future marketing campaigns should not be carried out during periods of economic crisis or downturn in economic activity Dashboard details The dashboard has a total of 13 pages, dark and light mode selector, segmentations, filters, bookmarks, page navigators, measures and many more things The first 4 pages are: front page, glossary and the navigation section (1 for dark mode and 1 for light mode). Each page has navigation bookmarks The next 8 pages are: actual campaign, last campaign, bank’s clients and socioeconomic indexes (4 for light mode and 4 for dark mode + 1 segmentation for light mode + 1 segmentation for dark mode) The last page summarizes all the work showing relevant points and recommendations for the bank’s managers " }, { "title": "Power BI dashboard - help desk tickets", "url": "/posts/pbi-udemy/", "categories": "Projects, PowerBI", "tags": "transform, data", "date": "2022-02-01 18:32:00 -0300", "snippet": "AboutThis dashboard summarizes what was learned during a specialized course on PowerBI from Udemy. Business Intelligence Fundamentals with Power BI - ETL in Power Query, DAX Formulas, Data Analysis with DashboardsThe final result is almost identical to that of the instructor since the idea was to follow his actions step by step.TopicThe chosen dataset is about a help desk ticket database with names, scores, dates, and much more.FeaturesThe dataset contains more than 20 attributes, more than 95K rows and the information is not real. All the inputs are from Jan 2016 to December 2020We can divide all attributes in 4 groups: Tickets (category, date, etc) Level of satisfaction (rating, groups, etc) Employees (users of help desk ticket system) Calendar Agents (help desk employees) Dashboard details The dashboard has a total of 4 pages, dark and light mode selector, segmentations, filters, bookmarks, page navigators, measures and many more things The first 2 pages are: tickets from 2016 and 2020 (light and dark mode) The next 2 pages are: ranking table and a top 3 ranking with pictures and filters " }, { "title": "Six problem types", "url": "/posts/six-problems/", "categories": "Blogging, Data", "tags": "objectives, transform, data", "date": "2022-01-29 18:32:00 -0300", "snippet": "Data analytics is so much more than just plugging information into a platform to find insights. It is about solving problems.To get to the root of these problems and find practical solutions, there are lots of opportunities for creative thinking. No matter the problem, the first and most important step is understanding it.1. Making predictionsAnalysts with data on location, type of media, and number of new customers acquired as a result of past ads can help predict the best placement of advertising to reach the target audience.2. Categorizing thingsAn example is a company’s goal to improve customer satisfaction. Analysts might classify customer service calls based on certain keywords or scores. This could help identify top-performing customer service representatives or help correlate certain actions taken with higher customer satisfaction scores.3. Spotting something unusualAnalysts who have analyzed aggregated health data can help product developers determine the right algorithms to spot and set off alarms when certain data doesn’t trend normally.4. Identifying themesUX designers might rely on analysts to analyze user interaction data. Usability improvement projects might require analysts to identify themes to help prioritize the right product features for improvement. In a user study, user beliefs, practices, and needs are examples of themes.Categorizing things involves assigning items to categories; identifying themes takes those categories a step further by grouping them into broader themes.5. Discovering connectionsA third-party logistics company working with another company to get shipments delivered to customers on time is a problem requiring analysts to discover connections. By analyzing the wait times at shipping hubs, analysts can determine the appropriate schedule changes to increase the number of on-time deliveries.6. Finding patternsMinimizing downtime caused by machine failure is an example of a problem requiring analysts to find patterns in data. For example, by analyzing maintenance data, they might discover that most failures happen if regular maintenance is delayed by more than a 15-day window." }, { "title": "Transforming data into insights", "url": "/posts/data-transforming/", "categories": "Blogging, Data", "tags": "objectives, transform, data", "date": "2022-01-29 18:32:00 -0300", "snippet": "You can find data pretty much everywhere. Any time you observe and evaluate something in the world, you’re collecting and analyzing data.There are six data analysis phases or steps: ask, prepare, process, analyze, share, and act. Following them should result in a frame that makes decision-making and problem solving a little easier. Your analysis helps you find easier ways of doing things, identify patterns to save you time, and discover surprising new perspectives that can completely change the way you experience things.flowchart TB; subgraph Transforming data into insights; Ask--&amp;gt;Prepare--&amp;gt;Process--&amp;gt;Analyze--&amp;gt;Share--&amp;gt;Act!; end;AskFirst up, you need to define what the project would look like and what would qualify as a successful result. To determine these things, ask effective questions and collaborate with leaders and managers who are interested in the outcome of their analysis. Check these examples Have you gathered data from new employees before? If so, may we have access to the historical data? What do you suspect is a leading cause of dissatisfaction among new employees? By what percentage would you like employee retention to increase in the next fiscal year?PrepareDuring this step, the analyst identifies what data is needed to achieve the successful result they identified in the previous step. This step can also involve things like rules for who would have access to the data collected or what information would be gathered and how to present it visuallyProcessWhat we aim to achieve is clean data. And to tell the truth, that is a science on its own. There are plenty of tools, theories, and methods to use but sometimes even a simple Spreadsheet program will suffice. So during this step one might: Using proper tools to find incorrect, incomplete or inconsistence data. Identify whether your data is biased. Essentially, data that is biased will not be representative of the population or phenomenon of study, our issue we are trying to solve.AnalyzeNext up is to make some conclusions based on the trustable data. Main concept is to think analytically about your data, be critical and be creative. There might be a need to sort and format the data to make it easier to process, create graphs, performing different calculations or get additional metricsShareSo as the next step, get additional opinions about the findings. This will significantly help to improve the results and ensure that main aspects were taken into account. As the are many ways to share the finding each person has their preference and so does each company.ActThe last stage of the process, work with leaders/managers within the company and decide how best to implement changes and take actions based on the findings" }, { "title": "Ask SMART questions", "url": "/posts/data-smart/", "categories": "Blogging, Data", "tags": "objectives, smart, data", "date": "2022-01-29 18:32:00 -0300", "snippet": "Asking the right questions can help spark the innovative ideas that so many businesses are hungry for these days.The same goes for data analytics. No matter how much information you have or how advanced your tools are, your data won’t tell you much if you don’t start with the right questions.SpecificA specific objective is circumscribed to a certain aspect, task or action of a company. In marketing, a specific goal could be, for example, to increase the monthly generation of MQLs (Marketing Qualified Leads) by 20% (from 600 to 720). This goal is specific because it tells us exactly what we hope to accomplish.MeasurableTo be measurable, a goal has to be specific. Otherwise, it is not possible to interpret whether the results are within the expected. In addition, it is necessary to have the means to be able to measure itAchievableBy achievable we mean an objective that is perfectly achievable under the conditions that are available. For example, depending on the characteristics of the company and the market, trying to increase the monthly generation of MQLs by 75% could be too much. It is very important to set realistic goals, bearing in mind that based on their fulfillment, you can then aim higher.RelevantA relevant goal is one that is in line with the general objectives of the business. It makes no sense to consider actions whose results are not subsidiary to any of the general objectives that the company has in its development plan. In this sense, increasing the monthly generation of MQLs by 20% will be relevant to the extent that the company intends to increase its billing, and that a percentage of those MQLs can effectively translate into sales.Time boundThat SMART objectives are temporary means that they are limited to a certain time. All the characteristics mentioned -their specificity and measurability, their scope and relevance- depend on the time in which they must be completed. The time allotted to an objective may make it unrealizable, or may make it difficult to measure. Therefore, when we set a goal of this type, we must always take into account the period." } ]
